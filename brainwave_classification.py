# -*- coding: utf-8 -*-
"""Brainwave classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1go2CA0nuKP4gdBHX93a-rJvNlMFCTYjh
"""

!pip install mne
!pip install xgboost

from google.colab import drive
drive.mount('/content/drive')

import mne
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torch.autograd import Variable
import scipy.io
from sklearn.model_selection import StratifiedKFold
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
import seaborn as sb

sub=['MM05', 'MM08','MM09','MM10','MM11','MM12','MM14','MM15','MM16','MM18',
     'MM19','MM20','MM21','P02']

EEGFiles=['Acquisition 232 Data', 'Acquisition 261 Data', 'Acquisition 263 Data',
          'Acquisition 284 Data', 'Acquisition 283 Data', 'Acquisition 283 Data',
           'Acquisition 283 Data', 'Acquisition 283 Data', 'Acquisition 283 Data',
          'Acquisition 283 Data', 'Acquisition 283 Data', 'Acquisition 283 Data',
          'Acquisition 01 Data', 'EEG_SPEECH_TEST_20140530']

tagFiles=['MM05_p', 'MM08_p','MM09_p','MM10_p','MM11_p','MM12_p','MM14_p','MM15_p',
          'MM16_p','MM18_p','MM19_p','MM20_p','MM21_p','P02_p']

EEGPath='/content/drive/My Drive/p/spoclab/users/szhao/EEG/data/'
tagsPath='/content/drive/My Drive/p/spoclab/users/szhao/EEG/data/'
epochsPath='/content/drive/My Drive/p/spoclab/users/szhao/EEG/data/'

EEGChannels= ['FP1', 'FPZ', 'FP2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2',
       'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6',
       'FT8', 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', 'TP7', 'CP5', 'CP3',
       'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8', 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2',
       'P4', 'P6', 'P8', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', 'O1', 'OZ',
        'O2', 'CB1', 'CB2']

def getEEGdata(index):
    fileName= EEGPath+sub[index]+'/'+EEGFiles[index]+'.cnt'
    #import raw EEG file
    rawEEG = mne.io.read_raw_cnt(fileName, preload=True)
    #Plotting raw EEG data for 10 channels
    #rawEEG.plot(duration=5, n_channels=10, title="Raw EEG data plot")
    #Selecting only EEG channels (excluding EMG channels)
    rawEEG= rawEEG.pick_channels(EEGChannels)
    #Applying band-pass filtering from 1Hz to 50Hz
    rawEEG.filter(1., 50.,fir_design='firwin')
    #Plotting filtered EEG data for 10 channels
    #rawEEG.plot(duration=5, n_channels=10, title="Filtered EEG data plot")
    #obtaining data and time indexes
    data, times = rawEEG[:]
    return data, times

def getTags(index):
    fileName= tagsPath+ sub[index]+'/kinect_data/'+ tagFiles[index]+'.txt'
    tags = np.loadtxt(fileName, dtype=str)
    tags = np.delete(tags, np.argwhere(tags == 'gnaw'))
    tags = np.delete(tags, np.argwhere(tags == 'knew'))
    tags = np.delete(tags, np.argwhere(tags == 'pat'))
    tags = np.delete(tags, np.argwhere(tags == 'pot'))
    tags = np.delete(tags, np.argwhere(tags == '/tiy/'))
    tags = np.delete(tags, np.argwhere(tags == '/diy/'))
    tags = np.delete(tags, np.argwhere(tags == '/piy/'))
    end = np.shape(tags)[0]
    tags[(tags == '/uw/') ] = 0
    tags[(tags == '/iy/') ] = 0
    #tags[(tags == '/tiy/') ] = 1
    #tags[(tags == '/diy/') ] = 1
    #tags[(tags == '/piy/') ] = 1
    tags[(tags == '/m/') ] = 1
    tags[(tags == '/n/') ] = 1
    y = tags.astype(float)
    class_0 = sum(y==0)
    class_1 = sum(y==1)
    print("Printing trials info for sub ", index+1)
    print("+-----------------------------------------------------+")
    print("Number of class 0: ", class_0)
    print("Number of class 1: ", class_1)
    proportion_0= class_0/len(y)
    proportion_1= class_1/len(y)
    print("Proportion of class 0: ", proportion_0)
    print("Proportion of class 1: ", proportion_1)
    print("+-----------------------------------------------------+")
    return y, end

def getEpochingIndexes(index, end):
    fileName= epochsPath+sub[index]+'/epoch_inds.mat'
    epochIndicesData = scipy.io.loadmat(fileName)
    epochIndicesArray = epochIndicesData.get('clearing_inds')
    epochIndices = epochIndicesArray[:, :end]
    #print(np.shape(epochIndices))
    maxdiff = 0
    for i in range(end):
        temp = epochIndices[0,i]
        start = temp[0,0]
        ending = temp [0,1]
        diff = ending - start
        if diff> maxdiff:
            maxdiff = diff
    return epochIndices, maxdiff

def getEpochedCCVData(data, epochIndices, maxdiff):
    epochedCCV = []
    for i in range(np.shape(epochIndices)[1]):
        temp = epochIndices[0,i]
        start = temp[0,0]
        ending = temp [0,1]
        trial= data[:, start:start+maxdiff]
        CCV = np.cov(trial, bias=True)
        epochedCCV.append(CCV)
    X = np.array(epochedCCV)
    return X

def get_X_y_sub(index):
    data, times = getEEGdata(index)
    y, end= getTags(index)
    epochIndices, maxdiff= getEpochingIndexes(index, end)
    X= getEpochedCCVData(data, epochIndices, maxdiff)
    return X, y

X,y = get_X_y_sub(0)
#for i in range(1,14):
for i in range(1,3):
    X_sub, y_sub = get_X_y_sub(i)
    X = np.concatenate((X, X_sub), axis=0)
    y = np.concatenate((y, y_sub), axis=0)

print(np.shape(X))

"""1232 = trials

62 = channels

(1232 x 62 x 62) => Channel Cross Covariance Matrix for 62 channels for a total of 1232 trials
"""

print(np.shape(y))

"""1232 = trials

(1232, ) => Tags for 1232 trials
"""

class_0 = sum(y==0)
class_1 = sum(y==1)
proportion_0= class_0/len(y)
proportion_1= class_1/len(y)

print("Printing trials info for entire datatset")
print("+-----------------------------------------------------+")
print("Number of class 0: ", class_0)
print("Number of class 1: ", class_1)
print("Proportion of class 0: ", proportion_0)
print("Proportion of class 1: ", proportion_1)
print("+-----------------------------------------------------+")

class MyDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = torch.LongTensor(targets)

    def __getitem__(self, index):
        x = self.data[index]
        y = self.targets[index]
        return x, y

    def __len__(self):
        return len(self.data)

"""# NETWORK ARCHITECTURE"""

class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.drop1 = nn.Dropout(0.25)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.drop2 = nn.Dropout(0.50)
        self.fc1 = nn.Linear(64*58*58, 128)
        self.fc2 = nn.Linear(128, 2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.drop1(x)
        x = F.relu(self.conv2(x))
        x = self.drop2(x)
        x = x.view(x.size(0), -1)
        x_out = F.relu(self.fc1(x))
        x = self.fc2(x_out)
        return x, x_out

class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size=62*62, hidden_size=128, num_layers=1,
                             batch_first=True, dropout=0.25)
        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1,
                             batch_first=True, dropout=0.50)
        self.fc1 = nn.Linear(256, 1024)   #what about 512?
        self.fc2 = nn.Linear(1024, 2)

    def forward(self, x):
        x, hidden = self.lstm1(x)
        x = F.relu(x)
        x, hidden = self.lstm2(x)
        x = F.relu(x)
        #x = x.view(x.size(0), -1)
        x_out = F.relu(self.fc1(x))
        x = self.fc2(x_out)
        return x, x_out

class DAE(nn.Module):
  def __init__(self):
    super(DAE, self).__init__()
    self.encoder = nn.Sequential(
        nn.Linear(1152, 512),
        nn.ReLU(),
        nn.Dropout(0.25),
        nn.Linear(512, 128),
        nn.ReLU(),
        nn.Dropout(0.25),
        nn.Linear(128, 32),
        nn.Sigmoid(),
        nn.Dropout(0.25)
    )
    self.decoder = nn.Sequential(
        nn.Linear(32, 128),
        nn.Sigmoid(),
        nn.Dropout(0.25),
        nn.Linear(128, 512),
        nn.ReLU(),
        nn.Dropout(0.25),
        nn.Linear(512, 1152),
        nn.Tanh(),
        nn.Dropout(0.25)
    )

  def forward(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

def trainCNN(epoch, training_loader):
    predicted_labels = []
    modelCNN.train()
    for i, data in enumerate(training_loader):
        # Every data instance is an input + label pair
        train_X, train_y = data
        train_X = train_X.reshape(train_X.shape[0], 1, 62, 62)
        train_X = train_X.float()
        train_y = train_y.long()

        # getting the training set
        X_train, y_train = Variable(train_X), Variable(train_y)
        # converting the data into GPU format
        if torch.cuda.is_available():
            X_train = X_train.cuda()
            y_train = y_train.cuda()

        # clearing the Gradients of the model parameters
        optimizerCNN.zero_grad()

        # prediction for training and test set
        output_train, CNN_out_train = modelCNN(X_train)

        # computing the training loss
        loss_train = criterion(output_train, y_train)

        # computing the updated weights of all the model parameters
        loss_train.backward()
        optimizerCNN.step()

    #if epoch%2 == 0:
        #print('Epoch : ',epoch+1, ' loss :', loss_test, ' accuracy :', test_accuracy)
    return modelCNN
    #return accuracy, cm, f1, CNN_out_train, CNN_out_test

def trainLSTM(epoch, training_loader):
    predicted_labels = []
    modelLSTM.train()
    for i, data in enumerate(training_loader):
        # Every data instance is an input + label pair
        train_X, train_y = data
        train_X = train_X.reshape([train_X.shape[0], 1, 62*62])
        train_X = train_X.float()
        train_y = train_y.long()
        # getting the training set
        X_train, y_train = Variable(train_X), Variable(train_y)
        # converting the data into GPU format
        if torch.cuda.is_available():
            X_train = X_train.cuda()
            y_train = y_train.cuda()

        # clearing the Gradients of the model parameters
        optimizerLSTM.zero_grad()

        # prediction for training and test set
        output_train, LSTM_out_train = modelLSTM(X_train)
        output_train = torch.reshape(output_train, (output_train.shape[0],2))

        # computing the training loss
        loss_train = criterion(output_train, y_train)

        # computing the updated weights of all the model parameters
        loss_train.backward()
        optimizerLSTM.step()

    #if epoch%2 == 0:
        #print('Epoch : ',epoch+1, ' loss :', loss_test, ' accuracy :', test_accuracy)
    return modelLSTM
    #return accuracy, cm, f1, LSTM_out_train, LSTM_out_test

def trainDAE(epoch):
  # getting the training set
  train_DAE = Variable(DAE_train)
  # getting the test set
  test_DAE = Variable(DAE_test)
  # converting the data into GPU format
  if torch.cuda.is_available():
    train_DAE = train_DAE.cuda()
    test_DAE = test_DAE.cuda()

  # clearing the Gradients of the model parameters
  optimizerDAE.zero_grad()

  # getting the reconstructed train, test values after putting into the DAE
  reconstructed_train = modelDAE(train_DAE)
  reconstructed_test = modelDAE(test_DAE)

  # computing training and testing loss
  loss_train = criterion_DAE(reconstructed_train, train_DAE)
  loss_test = criterion_DAE(reconstructed_test, test_DAE)

  # computing the updated weights of all the model parameters
  loss_train.backward()
  optimizerDAE.step()

  #if epoch%2 == 0:
    #print('Epoch : ',epoch+1, ' loss :', loss_test)

  return loss_train, loss_test, reconstructed_train, reconstructed_test

def trainXGB():
  train = xgb.DMatrix(XGB_train_X, label= XGB_train_y)
  test = xgb.DMatrix(XGB_test_X, label= XGB_test_y)
  param = {
      'booster': 'gbtree',
      'max_depth': 10,
      'n_estimators': 5000,
      'learning_rate': 0.1,
      #'reg_alpha': 0.3,
      #'reg_lambda': 0.3,
      'subsample': 0.8,
      #'colsample_bytree': 0.4, #by tree
      'colsample_bylevel': 0.4, #by level
      #'colsample_bynode': 0.4, #by node
      'objective': 'multi:softmax',
      'num_class': 2
      }
  epochs = 50
  model = xgb.train(param, train, epochs)
  predictions_train = model.predict(train)
  predictions_test = model.predict(test)
  accuracy_train = sum(predictions_train==XGB_train_y)/len(XGB_train_y)
  accuracy_test = sum(predictions_test==XGB_test_y)/len(XGB_test_y)

  #computing confusion matrix and F1 score
  cm = confusion_matrix(XGB_test_y, predictions_test)
  f1 = f1_score(XGB_test_y, predictions_test)

  #calculating average loss for batch
  print("XGB")
  print("output size: ", predictions_train.shape)
  print("For training:")
  print("Batch accuracy: ", accuracy_train)
  print("For testing:")
  print("Batch accuracy: ", accuracy_test)
  return predictions_train, predictions_test, accuracy_test, cm, f1

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
torch.cuda.empty_cache()

# Hyper-parameters
#num_epochs = 50
num_epochs = 40
num_epochs_DAE = 200
batch_size = 64
learning_rate = 0.001

#instantiating the model
modelCNN = ConvNet().to(device)
modelLSTM = LSTM().to(device)
modelDAE = DAE().to(device)

#setting model loss function and optimizer
criterion = nn.CrossEntropyLoss()
criterion_DAE = nn.MSELoss()
optimizerCNN = torch.optim.Adam(modelCNN.parameters(), lr=learning_rate)
optimizerLSTM = torch.optim.Adam(modelLSTM.parameters(), lr=learning_rate)
optimizerDAE = torch.optim.Adam(modelDAE.parameters(), lr=learning_rate)

print(modelCNN)
print(modelLSTM)
print(modelDAE)

def model_fit_CNN(training_loader, train_X, test_X, test_y):

    # training the model
    for epoch in range(num_epochs):
        modelCNN = trainCNN(epoch, training_loader)

    modelCNN.eval()

    #  getting inputs ready to get the final train and test outputs from trained model
    X_train, X_test, y_test = Variable(train_X), Variable(test_X), Variable(test_y)
    # converting the data into GPU format
    if torch.cuda.is_available():
        X_train = X_train.cuda()
        X_test = X_test.cuda()
        y_test = y_test.cuda()
    output_train, CNN_out_train = modelCNN(X_train)
    output_test, CNN_out_test = modelCNN(X_test)
    #Performance metrics

    predicted_labels_test = torch.argmax(output_test, dim=1)
    accuracy = sum(predicted_labels_test==y_test)/len(y_test)
    predicted_labels_test = predicted_labels_test.cpu().detach().numpy()
    y_test = y_test.cpu().detach().numpy()
    cm = confusion_matrix(y_test, predicted_labels_test)
    f1 = f1_score(y_test, predicted_labels_test)


    return accuracy, cm, f1, CNN_out_train, CNN_out_test

def model_fit_LSTM(training_loader, train_X, test_X, test_y):

    # training the model
    for epoch in range(num_epochs):
        modelLSTM = trainLSTM(epoch, training_loader)

    modelLSTM.eval()

    #  getting inputs ready to get the final train and test outputs from trained model
    X_train, X_test, y_test = Variable(train_X), Variable(test_X), Variable(test_y)
    # converting the data into GPU format
    if torch.cuda.is_available():
        X_train = X_train.cuda()
        X_test = X_test.cuda()
        y_test = y_test.cuda()
    output_train, LSTM_out_train = modelLSTM(X_train)
    output_test, LSTM_out_test = modelLSTM(X_test)
    #Performance metrics

    #y_test = y_test.reshape(y_test.shape[0],2)
    predicted_labels_test = torch.argmax(output_test, dim=1)
    accuracy = sum(predicted_labels_test==y_test)/len(y_test)
    predicted_labels_test = predicted_labels_test.cpu().detach().numpy()
    y_test = y_test.cpu().detach().numpy()
    cm = confusion_matrix(y_test, predicted_labels_test)
    f1 = f1_score(y_test, predicted_labels_test)

    return accuracy, cm, f1, LSTM_out_train, LSTM_out_test

def model_fit_DAE():
  # empty list to store training and test losses
  train_losses, test_losses = [], []
  # empty list to store training and test DAE_out
  train_DAE_out, test_DAE_out = [], []

  # training the model
  for epoch in range(num_epochs_DAE):
    train_loss, test_loss, trainDAE_out, testDAE_out = trainDAE(epoch)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_DAE_out.append(trainDAE_out)
    test_DAE_out.append(testDAE_out)

  #calculating average loss for batch
  batch_train_loss, batch_test_loss = sum(train_losses)/len(train_losses), sum(test_losses)/len(test_losses)
  print("DAE")
  print("output size: ", train_DAE_out[0].shape)
  print("For training:")
  print("Batch loss: ", batch_train_loss)
  print("For testing:")
  print("Batch loss: ", batch_test_loss)
  return batch_train_loss, batch_test_loss, train_DAE_out[-1], test_DAE_out[-1]
  # which epoch's LSTM out to return for that batch?

skf = StratifiedKFold(n_splits=10, shuffle=True)

#list to store test accuracy for each split
CNN_accuracy1=[]
LSTM_accuracy1=[]
DAE_loss1=[]
XGB_accuracy1=[]

for train_index, test_index in skf.split(X,y):
    torch.cuda.empty_cache()
    #print("train index size: ", train_index.size)
    #print("test index size: ", test_index.size)
    #print("TRAIN INDICES: ", train_index, "TEST INDICES: ", test_index)

    #getting data split into test and train
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    #weighted random sampler to remove imbalance in  mini batch
    #print('y train tags 0/1: {}/{}'.format(len(np.where(y_train == 0)[0]), len(np.where(y_train == 1)[0])))
    class_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])
    weight = 1. / class_count
    samples_weight=[]
    for t in y_train:
        if t==0:
            samples_weight.append(weight[0])
        else:
            samples_weight.append(weight[1])
    samples_weight = np.array(samples_weight)
    samples_weight=torch.from_numpy(samples_weight)
    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))

    #creating a custom torch Dataset with training array
    #using dataloader for train data

    training_data = MyDataset(X_train, y_train)
    training_dataloader = DataLoader(training_data, batch_size=64, sampler=sampler)

    #getting dataloader inputs ready for CNN network

    train_X_CNN = X_train.reshape(X_train.shape[0], 1, 62, 62)
    test_X_CNN = X_test.reshape(X_test.shape[0], 1, 62, 62)
    train_X_CNN = torch.from_numpy(train_X_CNN)
    train_X_CNN = train_X_CNN.float()
    test_X_CNN = torch.from_numpy(test_X_CNN)
    test_X_CNN = test_X_CNN.float()
    train_y = torch.from_numpy(y_train).long()
    test_y = torch.from_numpy(y_test).long()
    #print("class 0 train: ", sum(train_y==0)/len(train_y), "class 1 train: ", sum(train_y==1)/len(train_y))
    #print("class 0 test: ", sum(test_y==0)/len(test_y), "class 1 test: ", sum(test_y==1)/len(test_y))
    CNN_accuracy, CNN_cm, CNN_f1, CNN_train_out, CNN_test_out = model_fit_CNN(training_dataloader, train_X_CNN,  test_X_CNN,  test_y)
    CNN_accuracy1.append(CNN_accuracy)
    print("CNN CONFUSION MATRIX")
    print(CNN_cm)
    ax = sb.heatmap(CNN_cm, annot=True, cmap='Blues')
    ax.set_title('Confusion Matrix for CNN classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['1','0'])
    ax.yaxis.set_ticklabels(['1','0'])
    plt.show()
    print("CNN F1 Score: ", CNN_f1)

    '''#getting dataloader inputs ready for LSTM network
    train_X_LSTM = X_train.reshape([X_train.shape[0], 1, 62*62])
    test_X_LSTM = X_test.reshape([X_test.shape[0], 1, 62*62])
    train_X_LSTM = torch.from_numpy(train_X_LSTM)
    train_X_LSTM = train_X_LSTM.float()
    test_X_LSTM = torch.from_numpy(test_X_LSTM)
    test_X_LSTM = test_X_LSTM.float()
    train_y = torch.from_numpy(y_train).long()
    test_y = torch.from_numpy(y_test).long()
    LSTM_accuracy, LSTM_cm, LSTM_f1, LSTM_train_out, LSTM_test_out = model_fit_LSTM(training_dataloader, train_X_LSTM, test_X_LSTM, test_y)
    LSTM_accuracy1.append(LSTM_accuracy)
    print("LSTM CONFUSION MATRIX")
    print(LSTM_cm)
    ax = sb.heatmap(LSTM_cm, annot=True, cmap='Reds')
    ax.set_title('Confusion Matrix for LSTM classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['1','0'])
    ax.yaxis.set_ticklabels(['1','0'])
    plt.show()
    print("LSTM F1 Score: ", LSTM_f1)

    #getting inputs ready for the DAE
    LSTM_train_out_reshaped = LSTM_train_out.reshape(LSTM_train_out.shape[0], LSTM_train_out.shape[2])
    LSTM_test_out_reshaped = LSTM_test_out.reshape(LSTM_test_out.shape[0], LSTM_test_out.shape[2])
    DAE_train = torch.cat((CNN_train_out, LSTM_train_out_reshaped), 1)
    DAE_test = torch.cat((CNN_test_out, LSTM_test_out_reshaped), 1)
    DAE_batch_loss_train, DAE_batch_loss_test, DAE_train_out, DAE_test_out = model_fit_DAE()
    DAE_loss1.append(DAE_batch_loss_test)

    #getting inputs ready for the XGBoost layer
    XGB_train_X = DAE_train_out.cpu().detach().numpy()
    XGB_train_y = train_y.cpu().detach().numpy()
    XGB_test_X = DAE_test_out.cpu().detach().numpy()
    XGB_test_y = test_y.cpu().detach().numpy()
    XGB_train_out, XGB_test_out, XGB_batch_accuracy, XGB_batch_cm, XGB_batch_f1 = trainXGB()
    XGB_accuracy1.append(XGB_batch_accuracy)
    print("OVERALL CONFUSION MATRIX")
    print(XGB_batch_cm)
    ax = sb.heatmap(XGB_batch_cm, annot=True, cmap='Greens')
    ax.set_title('Confusion Matrix for Overall classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['0','1'])
    ax.yaxis.set_ticklabels(['0','1'])
    plt.show()
    print("OVERALL F1 Score: ", XGB_batch_f1)'''

#Plotting CNN accuracy
x = np.arange(1, len(CNN_accuracy1)+1)
y_new = []
for i in range(0,len(CNN_accuracy1)):
  temp = CNN_accuracy1[i].cpu()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("CNN accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("CNN accuracy")
plt.plot(x, y, color ="red")
plt.show()

#printing max CNN classification accuracy
print("Peak CNN classification accuracy: ", np.amax(y))

#Plotting LSTM accuracy
x = np.arange(1, len(LSTM_accuracy1)+1)
y_new = []
for i in range(0,len(LSTM_accuracy1)):
  temp = LSTM_accuracy1[i].cpu()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("LSTM accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("LSTM accuracy")
plt.plot(x, y, color ="red")
plt.show()

#printing max LSTM classification accuracy
print("Peak LSTM classification accuracy: ", np.amax(y))

#Plotting DAE loss
x = np.arange(1, len(DAE_loss1)+1)
y_new = []
for i in range(0,len(DAE_loss1)):
  temp = DAE_loss1[i].cpu().detach()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("DAE loss for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("DAE loss")
plt.plot(x, y, color ="red")
plt.show()

#printing min DAE loss
print("Min DAE loss: ", np.amin(y))

#Plotting XGB/Overall network accuracy
x = np.arange(1, len(XGB_accuracy1)+1)
y = np.array(XGB_accuracy1)

# plotting
plt.title("Overall classification accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("Accuracy")
plt.plot(x, y, color ="green")
plt.show()

#printing max overall classification accuracy
print("Peak classification accuracy: ", np.amax(y))

"""# METHOD 2: LEAVE ONE SUBJECT OUT CROSS VALIDATION"""

def get_X_y_subject_train(sub):
  subjects = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]
  subjects.remove(sub)
  X,y = get_X_y_sub(subjects[0])
  for i in range(1,13):
    X_sub, y_sub = get_X_y_sub(subjects[i])
    X = np.concatenate((X, X_sub), axis=0)
    y = np.concatenate((y, y_sub), axis=0)
  return X, y

CNN_accuracy2=[]
LSTM_accuracy2=[]
DAE_loss2=[]
XGB_accuracy2=[]

for i in range(0,14):
    torch.cuda.empty_cache()
    X_test, y_test = get_X_y_sub(i)
    X_train, y_train = get_X_y_subject_train(i)

    #weighted random sampler to remove imbalance in  mini batch
    #print('y train tags 0/1: {}/{}'.format(len(np.where(y_train == 0)[0]), len(np.where(y_train == 1)[0])))
    class_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])
    weight = 1. / class_count
    samples_weight=[]
    for t in y_train:
        if t==0:
            samples_weight.append(weight[0])
        else:
            samples_weight.append(weight[1])
    samples_weight = np.array(samples_weight)
    samples_weight=torch.from_numpy(samples_weight)
    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))

    #creating a custom torch Dataset with training and test arrays
    #using dataloader for test data and train data
    training_data = MyDataset(X_train, y_train)
    training_dataloader = DataLoader(training_data, batch_size=64, sampler=sampler)
    test_data = MyDataset(X_test, y_test)
    test_dataloader = DataLoader(test_data, batch_size=64)

    #getting dataloader inputs ready for CNN network
    train_features, train_labels = next(iter(training_dataloader))
    #test_features, test_labels = next(iter(test_dataloader))
    train_X_CNN = train_features.reshape(64, 1, 62, 62)
    train_X_CNN = train_X_CNN.float()
    test_X_CNN = test_features.reshape(64, 1, 62, 62)
    test_X_CNN = test_X_CNN.float()
    train_y = train_labels.long()
    test_y = test_labels.long()
    #print("class 0 train: ", sum(train_y==0)/len(train_y), "class 1 train: ", sum(train_y==1)/len(train_y))
    #print("class 0 test: ", sum(test_y==0)/len(test_y), "class 1 test: ", sum(test_y==1)/len(test_y))
    CNN_batch_accuracy, CNN_batch_cm, CNN_batch_f1, CNN_train_out, CNN_test_out = model_fit_CNN()
    CNN_accuracy2.append(CNN_batch_accuracy)
    print("CNN CONFUSION MATRIX")
    print(CNN_batch_cm)
    ax = sb.heatmap(CNN_batch_cm, annot=True, cmap='Blues')
    ax.set_title('Confusion Matrix for CNN classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['0','1'])
    ax.yaxis.set_ticklabels(['0','1'])
    plt.show()
    print("CNN F1 Score: ", CNN_batch_f1)

    torch.cuda.empty_cache()
    #getting dataloader inputs ready for LSTM network
    train_features, train_labels = next(iter(training_dataloader))
    test_features, test_labels = next(iter(test_dataloader))
    train_X_LSTM = train_features.reshape([64, 1, 62*62])
    train_X_LSTM = train_X_LSTM.float()
    test_X_LSTM = test_features.reshape([64, 1, 62*62])
    test_X_LSTM = test_X_LSTM.float()
    train_y = train_labels.long()
    test_y = test_labels.long()
    LSTM_batch_accuracy, LSTM_batch_cm, LSTM_batch_f1, LSTM_train_out, LSTM_test_out = model_fit_LSTM()
    LSTM_accuracy2.append(LSTM_batch_accuracy)
    print("LSTM CONFUSION MATRIX")
    print(LSTM_batch_cm)
    ax = sb.heatmap(LSTM_batch_cm, annot=True, cmap='Reds')
    ax.set_title('Confusion Matrix for LSTM classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['0','1'])
    ax.yaxis.set_ticklabels(['0','1'])
    plt.show()
    print("LSTM F1 Score: ", LSTM_batch_f1)

    torch.cuda.empty_cache()
    #getting inputs ready for the DAE
    LSTM_train_out_reshaped = LSTM_train_out.reshape(LSTM_train_out.shape[0], LSTM_train_out.shape[2])
    LSTM_test_out_reshaped = LSTM_test_out.reshape(LSTM_test_out.shape[0], LSTM_test_out.shape[2])
    DAE_train = torch.cat((CNN_train_out, LSTM_train_out_reshaped), 1)
    DAE_test = torch.cat((CNN_test_out, LSTM_test_out_reshaped), 1)
    DAE_batch_loss_train, DAE_batch_loss_test, DAE_train_out, DAE_test_out = model_fit_DAE()
    DAE_loss2.append(DAE_batch_loss_test)

    torch.cuda.empty_cache()
    #getting inputs ready for the XGBoost layer
    XGB_train_X = DAE_train_out.cpu().detach().numpy()
    XGB_train_y = train_y.cpu().detach().numpy()
    XGB_test_X = DAE_test_out.cpu().detach().numpy()
    XGB_test_y = test_y.cpu().detach().numpy()
    XGB_train_out, XGB_test_out, XGB_batch_accuracy, XGB_batch_cm, XGB_batch_f1 = trainXGB()
    XGB_accuracy2.append(XGB_batch_accuracy)
    print("OVERALL CONFUSION MATRIX")
    print(XGB_batch_cm)
    ax = sb.heatmap(XGB_batch_cm, annot=True, cmap='Greens')
    ax.set_title('Confusion Matrix for Overall classifier\n\n');
    ax.set_xlabel('\nPredicted Values')
    ax.set_ylabel('Actual Values ');
    ax.xaxis.set_ticklabels(['0','1'])
    ax.yaxis.set_ticklabels(['0','1'])
    plt.show()
    print("OVERALL F1 Score: ", XGB_batch_f1)

#Plotting CNN accuracy
x = np.arange(1, len(CNN_accuracy2)+1)
y_new = []
for i in range(0,len(CNN_accuracy2)):
  temp = CNN_accuracy2[i].cpu()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("CNN accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("CNN accuracy")
plt.plot(x, y, color ="red")
plt.show()

#printing max CNN classification accuracy
print("Peak CNN classification accuracy: ", np.amax(y))

#Plotting LSTM accuracy
x = np.arange(1, len(LSTM_accuracy2)+1)
y_new = []
for i in range(0,len(LSTM_accuracy2)):
  temp = LSTM_accuracy2[i].cpu()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("LSTM accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("LSTM accuracy")
plt.plot(x, y, color ="red")
plt.show()

#printing max LSTM classification accuracy
print("Peak LSTM classification accuracy: ", np.amax(y))

#Plotting DAE loss
x = np.arange(1, len(DAE_loss2)+1)
y_new = []
for i in range(0,len(DAE_loss2)):
  temp = DAE_loss2[i].cpu().detach()
  y_new.append(temp)
y = np.array(y_new)

# plotting
plt.title("DAE loss for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("DAE loss")
plt.plot(x, y, color ="red")
plt.show()

#printing max CNN classification accuracy
print("Min DAE loss: ", np.amin(y))

#Plotting XGB/Overall network accuracy
x = np.arange(1, len(XGB_accuracy2)+1)
y = np.array(XGB_accuracy2)

# plotting
plt.title("Overall classification accuracy for 10 test train splits")
plt.xlabel("Test train split number")
plt.ylabel("Accuracy")
plt.plot(x, y, color ="green")
plt.show()

#printing max overall classification accuracy
print("Peak classification accuracy: ", np.amax(y))